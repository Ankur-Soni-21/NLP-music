{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nipun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nipun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nipun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\"topic\" refers to a set of related words or terms that represent \n",
    "#a coherent theme or concept present in a collection of text documents\n",
    "# Each topic is represented by a probability distribution over the vocabulary, \n",
    "#indicating the likelihood of each word occurring in that topic.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaababout</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abercrombie</th>\n",
       "      <th>abigail</th>\n",
       "      <th>abilities</th>\n",
       "      <th>absentminded</th>\n",
       "      <th>absinthe</th>\n",
       "      <th>absolute</th>\n",
       "      <th>...</th>\n",
       "      <th>youtubers</th>\n",
       "      <th>yup</th>\n",
       "      <th>zac</th>\n",
       "      <th>zero</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zuzus</th>\n",
       "      <th>éponine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Death and other</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacqueline</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kevin</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taylor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The iron claw</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 aaababout  abandon  abandoned  abdomen  abercrombie  abigail  \\\n",
       "Death and other          1        0          0        0            0        0   \n",
       "Jacqueline               0        0          0        1            3        1   \n",
       "Kevin                    0        3          1        0            0        0   \n",
       "Taylor                   0        0          1        0            0        0   \n",
       "The iron claw            0        0          0        0            0        0   \n",
       "\n",
       "                 abilities  absentminded  absinthe  absolute  ...  youtubers  \\\n",
       "Death and other          1             0         0         0  ...          0   \n",
       "Jacqueline               0             1         0         0  ...          0   \n",
       "Kevin                    0             0         1         0  ...          1   \n",
       "Taylor                   0             0         0         0  ...          0   \n",
       "The iron claw            0             0         0         3  ...          0   \n",
       "\n",
       "                 yup  zac  zero  zipper  zoomed  zuckerberg  zurich  zuzus  \\\n",
       "Death and other    0    0     0       0       0           0       1      0   \n",
       "Jacqueline         0    0     1       1       0           0       0      2   \n",
       "Kevin              0    0     0       0       1           2       0      0   \n",
       "Taylor             0    0     0       0       0           0       0      0   \n",
       "The iron claw      1    1     0       0       0           0       0      0   \n",
       "\n",
       "                 éponine  \n",
       "Death and other        0  \n",
       "Jacqueline             1  \n",
       "Kevin                  0  \n",
       "Taylor                 0  \n",
       "The iron claw          0  \n",
       "\n",
       "[5 rows x 5198 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('02D_dtm.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "#genism is used for topic modelling and finding similarity among large datasets\n",
    "from gensim import matutils, models\n",
    "#gives sparse matrix data structures as they are memory efficient\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Death and other</th>\n",
       "      <th>Jacqueline</th>\n",
       "      <th>Kevin</th>\n",
       "      <th>Taylor</th>\n",
       "      <th>The iron claw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaababout</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abdomen</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abercrombie</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Death and other  Jacqueline  Kevin  Taylor  The iron claw\n",
       "aaababout                  1           0      0       0              0\n",
       "abandon                    0           0      3       0              0\n",
       "abandoned                  0           0      1       1              0\n",
       "abdomen                    0           1      0       0              0\n",
       "abercrombie                0           3      0       0              0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()\n",
    "#in tdm rows represent words and column represent documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.matutils.Sparse2Corpus at 0x2199981f7c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "#converting it to genism compatible form\n",
    "#sparse matrix\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "#sparse matrix to genism convertible form\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1246: 'detailsseason',\n",
       " 1525: 'episode',\n",
       " 4697: 'title',\n",
       " 4756: 'tragicoriginal',\n",
       " 3710: 'release',\n",
       " 1689: 'february',\n",
       " 2247: 'hulu',\n",
       " 4755: 'tragic',\n",
       " 4152: 'sixth',\n",
       " 1245: 'details',\n",
       " 362: 'begins',\n",
       " 1949: 'glimpse',\n",
       " 4109: 'showcasing',\n",
       " 3872: 'rufuss',\n",
       " 22: 'acclaim',\n",
       " 1248: 'detective',\n",
       " 3462: 'present',\n",
       " 2300: 'imogenes',\n",
       " 1506: 'entanglements',\n",
       " 4477: 'sunil',\n",
       " 1181: 'deepen',\n",
       " 124: 'amidst',\n",
       " 287: 'backdrop',\n",
       " 1540: 'espionage',\n",
       " 4495: 'surveillance',\n",
       " 3153: 'orchestrated',\n",
       " 1500: 'enigmatic',\n",
       " 4953: 'viktor',\n",
       " 3909: 'sams',\n",
       " 2411: 'intricately',\n",
       " 2620: 'lays',\n",
       " 4025: 'series',\n",
       " 1557: 'events',\n",
       " 4878: 'unravel',\n",
       " 3979: 'secrets',\n",
       " 105: 'alliances',\n",
       " 3784: 'revelation',\n",
       " 4478: 'sunils',\n",
       " 1726: 'financial',\n",
       " 1237: 'desperation',\n",
       " 2623: 'leading',\n",
       " 4880: 'unsavory',\n",
       " 104: 'alliance',\n",
       " 4850: 'unexpected',\n",
       " 2958: 'murder',\n",
       " 97: 'alexandra',\n",
       " 702: 'character',\n",
       " 2385: 'intent',\n",
       " 1614: 'exposing',\n",
       " 1001: 'corporate',\n",
       " 2763: 'malfeasance',\n",
       " 3801: 'rich',\n",
       " 4825: 'twists',\n",
       " 5030: 'weaves',\n",
       " 4927: 'various',\n",
       " 4036: 'settings',\n",
       " 2388: 'interactions',\n",
       " 3392: 'poignant',\n",
       " 2523: 'karaoke',\n",
       " 4033: 'session',\n",
       " 2162: 'highlighting',\n",
       " 1236: 'despair',\n",
       " 4364: 'startling',\n",
       " 4959: 'violence',\n",
       " 3851: 'rooted',\n",
       " 2889: 'misdirected',\n",
       " 3592: 'quest',\n",
       " 2520: 'justice',\n",
       " 2707: 'llewellyn',\n",
       " 3190: 'overwhelmed',\n",
       " 2047: 'guilt',\n",
       " 4608: 'tension',\n",
       " 1536: 'escalates',\n",
       " 1299: 'discovery',\n",
       " 4191: 'slowpoisoning',\n",
       " 2171: 'hinting',\n",
       " 1185: 'deepseated',\n",
       " 956: 'conspiracy',\n",
       " 2430: 'involving',\n",
       " 2497: 'joining',\n",
       " 4903: 'upstart',\n",
       " 3489: 'private',\n",
       " 4247: 'solved',\n",
       " 2306: 'impossible',\n",
       " 1066: 'crime',\n",
       " 2949: 'mr',\n",
       " 3871: 'rufus',\n",
       " 1011: 'cotesworth',\n",
       " 1298: 'discover',\n",
       " 3000: 'necklace',\n",
       " 3204: 'paint',\n",
       " 176: 'applied',\n",
       " 4993: 'wallpaper',\n",
       " 1841: 'freshly',\n",
       " 3205: 'painted',\n",
       " 4994: 'walls',\n",
       " 106: 'allow',\n",
       " 1386: 'dry',\n",
       " 4388: 'sticking',\n",
       " 3210: 'palace',\n",
       " 3488: 'pristine',\n",
       " 3250: 'patch',\n",
       " 2898: 'missing',\n",
       " 1341: 'doorjamb',\n",
       " 3468: 'pressed',\n",
       " 969: 'contessas',\n",
       " 2982: 'nanny',\n",
       " 58: 'admitted',\n",
       " 4559: 'taped',\n",
       " 2710: 'lock',\n",
       " 5033: 'wednesday',\n",
       " 365: 'belgian',\n",
       " 344: 'beau',\n",
       " 4216: 'sneak',\n",
       " 2281: 'identified',\n",
       " 4648: 'thief',\n",
       " 3780: 'retrieve',\n",
       " 2783: 'market',\n",
       " 1816: 'fortunately',\n",
       " 4649: 'thieves',\n",
       " 1972: 'gossip',\n",
       " 1368: 'drinks',\n",
       " 1394: 'dumb',\n",
       " 25: 'accomplish',\n",
       " 3949: 'scotland',\n",
       " 5167: 'yard',\n",
       " 2182: 'hm',\n",
       " 5137: 'worlds',\n",
       " 2006: 'greatest',\n",
       " 5125: 'words',\n",
       " 5147: 'wouldbe',\n",
       " 4177: 'sleuths',\n",
       " 4024: 'separates',\n",
       " 4804: 'truth',\n",
       " 1007: 'cost',\n",
       " 2972: 'mysterious',\n",
       " 4634: 'theme',\n",
       " 4889: 'upbeat',\n",
       " 488: 'bossy',\n",
       " 3966: 'sea',\n",
       " 2299: 'imogene',\n",
       " 4310: 'spinning',\n",
       " 1059: 'credit',\n",
       " 1647: 'fairly',\n",
       " 683: 'certain',\n",
       " 3972: 'seasickness',\n",
       " 1767: 'floating',\n",
       " 4686: 'tiltawhirl',\n",
       " 1646: 'fairground',\n",
       " 3807: 'ride',\n",
       " 4311: 'spins',\n",
       " 3358: 'pivot',\n",
       " 4671: 'throwing',\n",
       " 1866: 'funnel',\n",
       " 595: 'cake',\n",
       " 3104: 'ohh',\n",
       " 4995: 'waltzer',\n",
       " 2854: 'metric',\n",
       " 865: 'colour',\n",
       " 4079: 'ship',\n",
       " 4186: 'sloshin',\n",
       " 3971: 'seas',\n",
       " 3566: 'purpose',\n",
       " 5164: 'ya',\n",
       " 730: 'cheese',\n",
       " 225: 'assistant',\n",
       " 2862: 'midlevel',\n",
       " 2784: 'marketing',\n",
       " 1585: 'executive',\n",
       " 4239: 'soft',\n",
       " 3620: 'rang',\n",
       " 2954: 'mum',\n",
       " 1419: 'earth',\n",
       " 1620: 'exterior',\n",
       " 4991: 'wall',\n",
       " 1176: 'deck',\n",
       " 117: 'ambient',\n",
       " 4602: 'temperature',\n",
       " 3084: 'ocean',\n",
       " 2527: 'keeps',\n",
       " 4031: 'servers',\n",
       " 993: 'cool',\n",
       " 1714: 'figured',\n",
       " 4266: 'source',\n",
       " 4747: 'traced',\n",
       " 191: 'argentina',\n",
       " 3065: 'november',\n",
       " 4750: 'tracks',\n",
       " 556: 'buenos',\n",
       " 89: 'aires',\n",
       " 4267: 'south',\n",
       " 122: 'american',\n",
       " 3135: 'operating',\n",
       " 3053: 'nose',\n",
       " 1553: 'evaded',\n",
       " 4846: 'undertaking',\n",
       " 3750: 'requires',\n",
       " 958: 'construction',\n",
       " 2759: 'maintenance',\n",
       " 4285: 'specialists',\n",
       " 2715: 'logistical',\n",
       " 1460: 'elephant',\n",
       " 3423: 'possible',\n",
       " 2153: 'hey',\n",
       " 3632: 'reached',\n",
       " 4973: 'voicemail',\n",
       " 4607: 'tense',\n",
       " 56: 'admire',\n",
       " 3898: 'safety',\n",
       " 3443: 'praise',\n",
       " 397: 'bigger',\n",
       " 3754: 'reset',\n",
       " 3924: 'scales',\n",
       " 2613: 'law',\n",
       " 1667: 'far',\n",
       " 2546: 'killing',\n",
       " 1012: 'cotesworths',\n",
       " 2528: 'keith',\n",
       " 4801: 'trubitsky',\n",
       " 2879: 'millionaire',\n",
       " 3409: 'poor',\n",
       " 3514: 'promised',\n",
       " 2659: 'lie',\n",
       " 3900: 'sail',\n",
       " 4442: 'styx',\n",
       " 149: 'annatripp',\n",
       " 3901: 'sailing',\n",
       " 4962: 'virgin',\n",
       " 4253: 'song',\n",
       " 973: 'continues',\n",
       " 2585: 'lady',\n",
       " 1855: 'fucked',\n",
       " 2723: 'lord',\n",
       " 3356: 'pitchy',\n",
       " 2907: 'mm',\n",
       " 3357: 'pity',\n",
       " 4592: 'teddy',\n",
       " 1267: 'dictionary',\n",
       " 2878: 'million',\n",
       " 885: 'commonly',\n",
       " 3248: 'passwords',\n",
       " 3504: 'program',\n",
       " 3624: 'rapid',\n",
       " 4452: 'succession',\n",
       " 2586: 'ladys',\n",
       " 3682: 'reflections',\n",
       " 5018: 'waves',\n",
       " 4276: 'spark',\n",
       " 2838: 'memory',\n",
       " 1846: 'friends',\n",
       " 1364: 'dreams',\n",
       " 4857: 'unharmed',\n",
       " 1891: 'gathering',\n",
       " 2100: 'harmony',\n",
       " 4794: 'tripp',\n",
       " 4141: 'singing',\n",
       " 1453: 'eleanor',\n",
       " 2927: 'month',\n",
       " 2217: 'horizon',\n",
       " 2779: 'marigold',\n",
       " 3075: 'obscene',\n",
       " 2895: 'miss',\n",
       " 3952: 'scotts',\n",
       " 113: 'alright',\n",
       " 1434: 'edgar',\n",
       " 390: 'bhandari',\n",
       " 1789: 'following',\n",
       " 2764: 'malta',\n",
       " 4336: 'spy',\n",
       " 1697: 'feelings',\n",
       " 2288: 'ii',\n",
       " 3196: 'owns',\n",
       " 460: 'boat',\n",
       " 3264: 'pays',\n",
       " 2761: 'maker',\n",
       " 314: 'bankrupted',\n",
       " 532: 'bringing',\n",
       " 3616: 'ran',\n",
       " 3145: 'options',\n",
       " 4684: 'til',\n",
       " 1287: 'dirty',\n",
       " 5014: 'water',\n",
       " 24: 'accomplice',\n",
       " 1067: 'criminal',\n",
       " 83: 'agreed',\n",
       " 4509: 'swear',\n",
       " 2823: 'means',\n",
       " 3669: 'recruit',\n",
       " 2393: 'intermediary',\n",
       " 222: 'asset',\n",
       " 2769: 'manager',\n",
       " 5195: 'zurich',\n",
       " 136: 'andreas',\n",
       " 5087: 'windeler',\n",
       " 3974: 'seat',\n",
       " 814: 'client',\n",
       " 5082: 'willing',\n",
       " 4613: 'terms',\n",
       " 4195: 'small',\n",
       " 3747: 'request',\n",
       " 1788: 'followed',\n",
       " 4749: 'tracking',\n",
       " 1864: 'fund',\n",
       " 909: 'completion',\n",
       " 3511: 'project',\n",
       " 1579: 'exchange',\n",
       " 4589: 'technological',\n",
       " 4894: 'upgrades',\n",
       " 4289: 'specification',\n",
       " 1607: 'expires',\n",
       " 3753: 'reservations',\n",
       " 313: 'bankers',\n",
       " 5068: 'whod',\n",
       " 4816: 'turned',\n",
       " 4083: 'shits',\n",
       " 3868: 'rub',\n",
       " 4209: 'smug',\n",
       " 1633: 'faces',\n",
       " 3631: 'reach',\n",
       " 5141: 'worse',\n",
       " 1735: 'finished',\n",
       " 3737: 'repo',\n",
       " 510: 'brass',\n",
       " 3400: 'polished',\n",
       " 4423: 'stripped',\n",
       " 3164: 'original',\n",
       " 862: 'color',\n",
       " 3301: 'period',\n",
       " 3292: 'perfect',\n",
       " 4030: 'server',\n",
       " 1670: 'farms',\n",
       " 562: 'built',\n",
       " 2845: 'mess',\n",
       " 2423: 'investigation',\n",
       " 2340: 'informed',\n",
       " 3508: 'progresses',\n",
       " 2819: 'meaning',\n",
       " 3738: 'report',\n",
       " 4401: 'stopped',\n",
       " 4916: 'valletta',\n",
       " 2551: 'kindness',\n",
       " 2125: 'heart',\n",
       " 3125: 'oon',\n",
       " 2590: 'land',\n",
       " 0: 'aaababout',\n",
       " 2583: 'lading',\n",
       " 4677: 'tie',\n",
       " 687: 'chair',\n",
       " 3112: 'olives',\n",
       " 506: 'brads',\n",
       " 505: 'brad',\n",
       " 4147: 'sisters',\n",
       " 679: 'ceo',\n",
       " 107: 'allowance',\n",
       " 1857: 'fuckin',\n",
       " 1467: 'embarrassing',\n",
       " 669: 'celia',\n",
       " 773: 'chuns',\n",
       " 892: 'company',\n",
       " 330: 'basically',\n",
       " 1742: 'fix',\n",
       " 148: 'anna',\n",
       " 4170: 'sleep',\n",
       " 3613: 'raised',\n",
       " 4313: 'spirits',\n",
       " 2640: 'leila',\n",
       " 2676: 'likes',\n",
       " 3174: 'outsiders',\n",
       " 4586: 'technically',\n",
       " 1157: 'deadly',\n",
       " 4562: 'tapped',\n",
       " 1655: 'fam',\n",
       " 4342: 'ss',\n",
       " 531: 'bringin',\n",
       " 4116: 'shrimp',\n",
       " 4415: 'stress',\n",
       " 724: 'check',\n",
       " 396: 'bigass',\n",
       " 4217: 'sneaky',\n",
       " 3362: 'plan',\n",
       " 3006: 'negotiations',\n",
       " 2615: 'lawrence',\n",
       " 670: 'celias',\n",
       " 2740: 'lube',\n",
       " 861: 'colliers',\n",
       " 979: 'control',\n",
       " 4058: 'shanghai',\n",
       " 1120: 'dab',\n",
       " 2495: 'join',\n",
       " 4700: 'toby',\n",
       " 1223: 'derek',\n",
       " 5054: 'whatd',\n",
       " 1977: 'governor',\n",
       " 4280: 'speak',\n",
       " 4475: 'sunday',\n",
       " 2338: 'information',\n",
       " 860: 'collier',\n",
       " 1131: 'damning',\n",
       " 3522: 'proof',\n",
       " 3530: 'prosecute',\n",
       " 77: 'agent',\n",
       " 2168: 'hilde',\n",
       " 1533: 'eriksen',\n",
       " 2397: 'interpol',\n",
       " 297: 'badge',\n",
       " 4131: 'signing',\n",
       " 2186: 'hochenberg',\n",
       " 3748: 'requested',\n",
       " 2401: 'interview',\n",
       " 230: 'assurances',\n",
       " 2298: 'immunity',\n",
       " 3212: 'pales',\n",
       " 895: 'comparison',\n",
       " 2375: 'intel',\n",
       " 1504: 'ensure',\n",
       " 1268: 'dies',\n",
       " 3486: 'prison',\n",
       " 630: 'career',\n",
       " 3666: 'records',\n",
       " 3968: 'sealed',\n",
       " 4622: 'testify',\n",
       " 1332: 'documentation',\n",
       " 4887: 'untie',\n",
       " 2744: 'lunacy',\n",
       " 1348: 'double',\n",
       " 1406: 'duty',\n",
       " 1668: 'farm',\n",
       " 1164: 'decade',\n",
       " 1038: 'cover',\n",
       " 923: 'conduct',\n",
       " 2396: 'international',\n",
       " 416: 'blackmail',\n",
       " 3136: 'operation',\n",
       " 1465: 'elude',\n",
       " 1247: 'detection',\n",
       " 3818: 'risk',\n",
       " 2544: 'kill',\n",
       " 1137: 'danny',\n",
       " 5134: 'works',\n",
       " 5146: 'worthy',\n",
       " 1780: 'foe',\n",
       " 4726: 'tops',\n",
       " 379: 'bender',\n",
       " 3175: 'outta',\n",
       " 617: 'capable',\n",
       " 2936: 'motherfucker',\n",
       " 3587: 'quarters',\n",
       " 803: 'clear',\n",
       " 3936: 'scene',\n",
       " 133: 'ananother',\n",
       " 1155: 'dead',\n",
       " 3849: 'rooms',\n",
       " 3697: 'reinforcements',\n",
       " 4939: 'vessel',\n",
       " 4095: 'shore',\n",
       " 4104: 'shouldnt',\n",
       " 3431: 'potatoes',\n",
       " 1135: 'danger',\n",
       " 2267: 'hunting',\n",
       " 1420: 'earthly',\n",
       " 5066: 'whistleblower',\n",
       " 789: 'claiming',\n",
       " 641: 'case',\n",
       " 1564: 'evidence',\n",
       " 3202: 'pages',\n",
       " 420: 'blank',\n",
       " 2989: 'natural',\n",
       " 4508: 'swapped',\n",
       " 1565: 'evidently',\n",
       " 2545: 'killer',\n",
       " 2968: 'mustve',\n",
       " 2574: 'known',\n",
       " 3366: 'planning',\n",
       " 3533: 'protecting',\n",
       " 2616: 'lawyer',\n",
       " 2428: 'involved',\n",
       " 2622: 'lead',\n",
       " 2364: 'inspector',\n",
       " 1845: 'friedrich',\n",
       " 1249: 'deteriorated',\n",
       " 3244: 'passenger',\n",
       " 2524: 'kay',\n",
       " 2721: 'lookin',\n",
       " 3395: 'poison',\n",
       " 4395: 'stomach',\n",
       " 1973: 'gotten',\n",
       " 4426: 'stronger',\n",
       " 2745: 'lunch',\n",
       " 2025: 'gross',\n",
       " 4877: 'unprofesh',\n",
       " 2675: 'likely',\n",
       " 4181: 'slipped',\n",
       " 848: 'coffee',\n",
       " 440: 'blood',\n",
       " 2693: 'lips',\n",
       " 1015: 'cough',\n",
       " 4466: 'suggests',\n",
       " 240: 'attacked',\n",
       " 3762: 'respiratory',\n",
       " 55: 'administered',\n",
       " 441: 'bloodstream',\n",
       " 4511: 'sweating',\n",
       " 4999: 'warm',\n",
       " 5028: 'wearing',\n",
       " 4175: 'sleeves',\n",
       " 3002: 'necrotizing',\n",
       " 1671: 'fasciitis',\n",
       " 5160: 'wrote',\n",
       " 102: 'allergies',\n",
       " 457: 'board',\n",
       " 4969: 'vitamin',\n",
       " 1370: 'drips',\n",
       " 3407: 'pool',\n",
       " 305: 'ball',\n",
       " 3245: 'passes',\n",
       " 926: 'confession',\n",
       " 3538: 'prove',\n",
       " 829: 'closest',\n",
       " 3545: 'psychopath',\n",
       " 2089: 'happily',\n",
       " 3121: 'oneself',\n",
       " 5097: 'winnie',\n",
       " 1425: 'eaten',\n",
       " 2093: 'hardly',\n",
       " 4176: 'slept',\n",
       " 4543: 'taken',\n",
       " 1731: 'fingernails',\n",
       " 3595: 'quick',\n",
       " 3725: 'remorseless',\n",
       " 63: 'advantage',\n",
       " 2660: 'lies',\n",
       " 2155: 'hid',\n",
       " 1762: 'flip',\n",
       " 4645: 'theyd',\n",
       " 2451: 'iv',\n",
       " 1369: 'drip',\n",
       " 316: 'bar',\n",
       " 640: 'cart',\n",
       " 767: 'chucked',\n",
       " 3177: 'overboard',\n",
       " 925: 'confessed',\n",
       " 2525: 'keeley',\n",
       " 2569: 'knocked',\n",
       " 1636: 'factory',\n",
       " 3500: 'products',\n",
       " 3869: 'rubber',\n",
       " 3195: 'owner',\n",
       " 2012: 'greedy',\n",
       " 4932: 'ventilation',\n",
       " 976: 'contractions',\n",
       " 357: 'began',\n",
       " 386: 'benzene',\n",
       " 419: 'blamed',\n",
       " 818: 'climbed',\n",
       " 4549: 'tallest',\n",
       " 2515: 'jumped',\n",
       " 4565: 'targeted',\n",
       " 266: 'avenged',\n",
       " 1005: 'corruption',\n",
       " 2005: 'greater',\n",
       " 3649: 'reason',\n",
       " 2959: 'murdered',\n",
       " 697: 'changing',\n",
       " 4599: 'telling',\n",
       " 944: 'connected',\n",
       " 2486: 'jiangsu',\n",
       " 851: 'coincidences',\n",
       " 1366: 'dressed',\n",
       " 835: 'clues',\n",
       " 1716: 'figuring',\n",
       " 2431: 'inés',\n",
       " 1330: 'document',\n",
       " 743: 'china',\n",
       " 315: 'banned',\n",
       " 628: 'carcinogenic',\n",
       " 732: 'chemical',\n",
       " 4080: 'shipping',\n",
       " 3653: 'receipt',\n",
       " 2422: 'investigating',\n",
       " 1212: 'denial',\n",
       " 3263: 'paying',\n",
       " 2920: 'mom',\n",
       " 5006: 'warning',\n",
       " 2782: 'mark',\n",
       " 2902: 'mistakes',\n",
       " 1744: 'fixed',\n",
       " 3652: 'rebuilt',\n",
       " 1512: 'entire',\n",
       " 1958: 'goddamn',\n",
       " 3956: 'scratch',\n",
       " 356: 'beg',\n",
       " 1590: 'exgirlfriend',\n",
       " 2663: 'lifeline',\n",
       " 1039: 'covered',\n",
       " 3397: 'poisoning',\n",
       " 4123: 'sick',\n",
       " 1068: 'criminals',\n",
       " 4373: 'stealing',\n",
       " 2772: 'manipulate',\n",
       " 5149: 'wrapped',\n",
       " 4772: 'trauma',\n",
       " 4856: 'ungrateful',\n",
       " 1099: 'cunt',\n",
       " 2127: 'heartbroken',\n",
       " 2270: 'husband',\n",
       " 3415: 'port',\n",
       " 270: 'awaiting',\n",
       " 1328: 'docked',\n",
       " 3211: 'palermo',\n",
       " 3304: 'permitted',\n",
       " 3758: 'resolved',\n",
       " 2399: 'interrogation',\n",
       " 2942: 'mourning',\n",
       " 3444: 'pray',\n",
       " 370: 'believer',\n",
       " 384: 'benevolence',\n",
       " 4275: 'spare',\n",
       " 3445: 'prayer',\n",
       " 195: 'argument',\n",
       " 2575: 'kompromat',\n",
       " 3328: 'photos',\n",
       " 4948: 'videos',\n",
       " 2801: 'materials',\n",
       " 4256: 'soontobe',\n",
       " 1629: 'exwife',\n",
       " 3032: 'nights',\n",
       " 81: 'ago',\n",
       " 502: 'bra',\n",
       " 1784: 'folder',\n",
       " 4146: 'sister',\n",
       " 622: 'captionem',\n",
       " 4384: 'steve',\n",
       " 3963: 'scrubbed',\n",
       " 764: 'christ',\n",
       " 5079: 'wifi',\n",
       " 296: 'backup',\n",
       " 1906: 'generators',\n",
       " 3123: 'online',\n",
       " 2303: 'implement',\n",
       " 3537: 'protocols',\n",
       " 4499: 'sushi',\n",
       " 853: 'cold',\n",
       " 2824: 'meant',\n",
       " 4734: 'touch',\n",
       " 5013: 'watched',\n",
       " 5078: 'wifes',\n",
       " 4558: 'tape',\n",
       " 2369: 'instead',\n",
       " 996: 'copped',\n",
       " 2881: 'mills',\n",
       " 3396: 'poisoned',\n",
       " 5129: 'workers',\n",
       " 1041: 'coverup',\n",
       " 1978: 'governors',\n",
       " 403: 'binds',\n",
       " 3576: 'puzzle',\n",
       " 4246: 'solve',\n",
       " 3342: 'pieces',\n",
       " 827: 'closer',\n",
       " 4302: 'spent',\n",
       " 3559: 'punish',\n",
       " 153: 'answers',\n",
       " 2625: 'league',\n",
       " 4360: 'staring',\n",
       " 2896: 'missed',\n",
       " 2061: 'hack',\n",
       " 2556: 'kira',\n",
       " 1230: 'deserved',\n",
       " 1761: 'flinch',\n",
       " 2402: 'interviewed',\n",
       " 2159: 'hiding',\n",
       " 4431: 'stu',\n",
       " 1783: 'fold',\n",
       " 1865: 'funds',\n",
       " 5104: 'wise',\n",
       " 5144: 'worth',\n",
       " 4251: 'somethin',\n",
       " 3417: 'portobello',\n",
       " 206: 'art',\n",
       " 4394: 'stolen',\n",
       " 3887: 'russian',\n",
       " 438: 'bloke',\n",
       " 366: 'belgians',\n",
       " 2461: 'jade',\n",
       " 2246: 'huh',\n",
       " 2421: 'invention',\n",
       " 449: 'bluff',\n",
       " 450: 'bluffed',\n",
       " 2937: 'mothers',\n",
       " 4872: 'unmask',\n",
       " 3513: 'promise',\n",
       " 1829: 'fraud',\n",
       " 3277: 'pegged',\n",
       " 4496: 'survive',\n",
       " 88: 'aint',\n",
       " 1827: 'frank',\n",
       " 917: 'concludes',\n",
       " 3950: 'scott',\n",
       " 3278: 'pelleys',\n",
       " 3146: 'oracle',\n",
       " 85: 'ai',\n",
       " 2522: 'kaifu',\n",
       " 2635: 'lee',\n",
       " 1968: 'googles',\n",
       " 1444: 'efforts',\n",
       " 2646: 'lesley',\n",
       " 4350: 'stahls',\n",
       " 715: 'chatbots',\n",
       " 716: 'chatgpt',\n",
       " 4868: 'unknowns',\n",
       " 2636: 'legacy',\n",
       " 2829: 'media',\n",
       " 1867: 'furious',\n",
       " 4808: 'tucker',\n",
       " 635: 'carlson',\n",
       " 2403: 'interviewing',\n",
       " 3573: 'putin',\n",
       " 281: 'axis',\n",
       " 1463: 'elon',\n",
       " 2967: 'musk',\n",
       " 1408: 'dwarf',\n",
       " 868: 'combined',\n",
       " 236: 'atlanta',\n",
       " 2600: 'larry',\n",
       " 1727: 'finds',\n",
       " 4432: 'stuck',\n",
       " 3729: 'rental',\n",
       " 3593: 'questionable',\n",
       " 2614: 'lawn',\n",
       " 3168: 'ornament',\n",
       " 2475: 'jeff',\n",
       " 3475: 'price',\n",
       " 2601: 'larrys',\n",
       " 4500: 'susies',\n",
       " 407: 'birthday',\n",
       " 1926: 'gift',\n",
       " 4272: 'spade',\n",
       " 322: 'bargained',\n",
       " 1429: 'eccentric',\n",
       " 3009: 'neighbours',\n",
       " 2630: 'learns',\n",
       " 6: 'abilities',\n",
       " 4611: 'teresa',\n",
       " 2473: 'jeanpierre',\n",
       " 935: 'confronts',\n",
       " 3319: 'philippe',\n",
       " 2565: 'knees',\n",
       " 2460: 'jacqueline',\n",
       " 3063: 'novak',\n",
       " 4763: 'transcends',\n",
       " 4831: 'typical',\n",
       " 4356: 'standup',\n",
       " 873: 'comedy',\n",
       " 1204: 'delivering',\n",
       " 4863: 'unique',\n",
       " 426: 'blend',\n",
       " 138: 'anecdotes',\n",
       " 2377: 'intellectual',\n",
       " 1610: 'exploration',\n",
       " 3147: 'oral',\n",
       " 4765: 'transforming',\n",
       " 4633: 'theatrical',\n",
       " 1604: 'experience',\n",
       " 733: 'cherry',\n",
       " 2594: 'lane',\n",
       " 4632: 'theater',\n",
       " 4424: 'stripping',\n",
       " 3478: 'primal',\n",
       " 1459: 'elements',\n",
       " 4832: 'typically',\n",
       " 228: 'associated',\n",
       " 2426: 'invites',\n",
       " 257: 'audience',\n",
       " 681: 'cerebral',\n",
       " 1301: 'discussion',\n",
       " 4444: 'subject',\n",
       " 2259: 'humorously',\n",
       " 1312: 'dissecting',\n",
       " 4008: 'semantics',\n",
       " 1095: 'cultural',\n",
       " 3290: 'perceptions',\n",
       " 2521: 'juxtaposition',\n",
       " 3330: 'physical',\n",
       " 1233: 'desire',\n",
       " 132: 'analysis',\n",
       " 3296: 'performance',\n",
       " 704: 'characterized',\n",
       " 3996: 'selfdeprecation',\n",
       " 4067: 'sharp',\n",
       " 5108: 'wit',\n",
       " 3684: 'refusal',\n",
       " 930: 'conform',\n",
       " 984: 'conventional',\n",
       " 872: 'comedic',\n",
       " 4537: 'tactics',\n",
       " 3096: 'offers',\n",
       " 1840: 'fresh',\n",
       " 3308: 'perspective',\n",
       " 1900: 'gender',\n",
       " 1413: 'dynamics',\n",
       " 4045: 'sexual',\n",
       " 1599: 'expectations',\n",
       " 3103: 'oftencomical',\n",
       " 3641: 'reality',\n",
       " 1471: 'embodying',\n",
       " 1696: 'feeling',\n",
       " 1234: 'desiring',\n",
       " 4238: 'society',\n",
       " 1718: 'filled',\n",
       " 977: 'contradictions',\n",
       " 2359: 'insightful',\n",
       " 2645: 'lens',\n",
       " 2992: 'navigates',\n",
       " 911: 'complexities',\n",
       " 2283: 'identity',\n",
       " 1619: 'expression',\n",
       " 4659: 'thoughtprovoking',\n",
       " 3681: 'reflection',\n",
       " 12: 'absurdity',\n",
       " 4046: 'sexuality',\n",
       " 2973: 'mystery',\n",
       " 2863: 'midnight',\n",
       " 1238: 'despite',\n",
       " 5044: 'welcome',\n",
       " 2503: 'journey',\n",
       " 294: 'backstage',\n",
       " 2859: 'microphone',\n",
       " 1830: 'fraught',\n",
       " 3724: 'reminds',\n",
       " 1828: 'frankly',\n",
       " 4250: 'someones',\n",
       " 4728: 'torso',\n",
       " 3279: 'pelvis',\n",
       " 443: 'blow',\n",
       " 3935: 'scenarios',\n",
       " 2114: 'headed',\n",
       " 2085: 'hanging',\n",
       " 298: 'badly',\n",
       " 4839: 'uncertainty',\n",
       " 4609: 'tenuousness',\n",
       " 3602: 'quite',\n",
       " 4593: 'tedium',\n",
       " 1073: 'critic',\n",
       " 109: 'allows',\n",
       " 2758: 'maintain',\n",
       " 1276: 'dignity',\n",
       " 3225: 'parent',\n",
       " 4121: 'sibling',\n",
       " 915: 'concerned',\n",
       " 1573: 'example',\n",
       " 3453: 'prefers',\n",
       " 1335: 'doggy',\n",
       " 4441: 'style',\n",
       " 2227: 'hounds',\n",
       " 3420: 'position',\n",
       " 1231: 'deserves',\n",
       " 4612: 'term',\n",
       " 1997: 'gravitas',\n",
       " 3038: 'noble',\n",
       " 3421: 'positions',\n",
       " 2734: 'lover',\n",
       " 2294: 'immature',\n",
       " 2310: 'impressed',\n",
       " 3744: 'represents',\n",
       " 2806: 'mature',\n",
       " 2735: 'lovers',\n",
       " 1870: 'future',\n",
       " 3352: 'pioneers',\n",
       " 5052: 'west',\n",
       " 4739: 'tough',\n",
       " 2238: 'hubbins',\n",
       " 592: 'caboose',\n",
       " 5086: 'wind',\n",
       " 2526: 'keeping',\n",
       " 5182: 'young',\n",
       " 2985: 'nation',\n",
       " 1219: 'depends',\n",
       " 4259: 'sort',\n",
       " 3388: 'poetic',\n",
       " 4017: 'sensibility',\n",
       " 5004: 'warn',\n",
       " 5155: 'write',\n",
       " 3391: 'poetry',\n",
       " 859: 'college',\n",
       " 4695: 'tired',\n",
       " 957: 'constant',\n",
       " 1487: 'enchantment',\n",
       " 1103: 'curled',\n",
       " 5091: 'windowsills',\n",
       " 2962: 'muscles',\n",
       " 1048: 'cramping',\n",
       " 1387: 'drying',\n",
       " 5121: 'wonderment',\n",
       " 3983: 'seeing',\n",
       " 2930: 'moon',\n",
       " 2029: 'grow',\n",
       " 32: 'accustomed',\n",
       " 2931: 'moonlight',\n",
       " 1991: 'granted',\n",
       " 1937: 'girls',\n",
       " 3141: 'opportunity',\n",
       " 1810: 'form',\n",
       " 3031: 'nightmare',\n",
       " 2318: 'incarnate',\n",
       " 2280: 'ideas',\n",
       " 2655: 'level',\n",
       " 742: 'chin',\n",
       " 380: 'beneath',\n",
       " 2679: 'limbs',\n",
       " 884: 'common',\n",
       " 1701: 'female',\n",
       " 3891: 'sack',\n",
       " 4698: 'tits',\n",
       " 1608: 'explanation',\n",
       " 797: 'class',\n",
       " 4569: 'taters',\n",
       " 3905: 'sale',\n",
       " 3133: 'operate',\n",
       " 2840: 'mental',\n",
       " 4215: 'snapshot',\n",
       " 3452: 'prefer',\n",
       " 454: 'blurry',\n",
       " 2000: 'gray',\n",
       " 452: 'blur',\n",
       " 4138: 'simple',\n",
       " 4002: 'selfpreservation',\n",
       " 1818: 'forward',\n",
       " 1294: 'discarding',\n",
       " 1756: 'flesh',\n",
       " 1923: 'ghost',\n",
       " 4288: 'specifically',\n",
       " 4759: 'training',\n",
       " 5058: 'wheels',\n",
       " 645: 'cast',\n",
       " 1776: 'fly',\n",
       " 2775: 'manufacture',\n",
       " 4854: 'unfinished',\n",
       " 2832: 'meditation',\n",
       " 3441: 'practice',\n",
       " 954: 'consistent',\n",
       " 4315: 'spiritually',\n",
       " 2368: 'instantly',\n",
       " 4761: 'transcend',\n",
       " 1314: 'dissolve',\n",
       " 2687: 'linger',\n",
       " 2108: 'haunt',\n",
       " 2228: 'house',\n",
       " 3582: 'quaint',\n",
       " 4874: 'unoriginal',\n",
       " 3042: 'nonetheless',\n",
       " 4598: 'television',\n",
       " 1941: 'giving',\n",
       " 3505: 'programming',\n",
       " 752: 'choices',\n",
       " 4493: 'surrender',\n",
       " 3827: 'robust',\n",
       " 453: 'blurring',\n",
       " 1436: 'edges',\n",
       " 1862: 'fully',\n",
       " 209: 'articulating',\n",
       " 1733: 'fingertips',\n",
       " 3127: 'opacity',\n",
       " 2885: 'minimum',\n",
       " 2810: 'max',\n",
       " 3479: 'primarily',\n",
       " 3150: 'orb',\n",
       " 1715: 'figures',\n",
       " 705: 'characters',\n",
       " 1468: 'embarrassment',\n",
       " 890: 'community',\n",
       " 1218: 'depending',\n",
       " 3043: 'nonfiction',\n",
       " 3224: 'paranormal',\n",
       " 2664: 'lifelong',\n",
       " 2627: 'learner',\n",
       " 3151: 'orbs',\n",
       " 4770: 'transparent',\n",
       " 780: 'circles',\n",
       " 2670: 'light',\n",
       " 3083: 'occasionally',\n",
       " 169: 'appear',\n",
       " 3327: 'photograph',\n",
       " 4249: 'someonell',\n",
       " 2007: 'greatgrandmother',\n",
       " 4436: 'stuff',\n",
       " 3870: 'rude',\n",
       " 3752: 'resembling',\n",
       " 4027: 'seriousness',\n",
       " 4139: 'simply',\n",
       " 2768: 'managed',\n",
       " 1890: 'gather',\n",
       " 4414: 'strength',\n",
       " 72: 'afterlife',\n",
       " 779: 'circle',\n",
       " 950: 'consciousness',\n",
       " 684: 'certainly',\n",
       " 329: 'basic',\n",
       " 4061: 'shapes',\n",
       " 1305: 'dismissed',\n",
       " 4470: 'summarily',\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"04D_cv.pkl\", \"rb\"))\n",
    "#converting in dictionary\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "id2word\n",
    "#term id corresponding to word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in lda we need 4 things term document matrix, dictionary of location to term, number of topics and number of passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"kev\" + 0.007*\"hey\" + 0.007*\"kerry\" + 0.006*\"crowd\" + 0.005*\"cheering\" + 0.004*\"david\" + 0.004*\"von\" + 0.004*\"pam\" + 0.004*\"fritz\" + 0.004*\"son\"'),\n",
       " (1,\n",
       "  '0.011*\"blow\" + 0.010*\"penis\" + 0.005*\"sort\" + 0.005*\"teeth\" + 0.003*\"viktor\" + 0.003*\"sams\" + 0.003*\"balls\" + 0.003*\"dick\" + 0.003*\"cock\" + 0.003*\"boyfriend\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"kev\" + 0.005*\"wee\" + 0.005*\"cos\" + 0.004*\"mate\" + 0.004*\"viktor\" + 0.004*\"sams\" + 0.003*\"quite\" + 0.003*\"son\" + 0.003*\"telly\" + 0.003*\"everybody\"'),\n",
       " (1,\n",
       "  '0.015*\"blow\" + 0.013*\"penis\" + 0.006*\"teeth\" + 0.006*\"sort\" + 0.005*\"balls\" + 0.004*\"dick\" + 0.004*\"cock\" + 0.003*\"boyfriend\" + 0.003*\"bite\" + 0.003*\"rockhard\"'),\n",
       " (2,\n",
       "  '0.011*\"hey\" + 0.010*\"kerry\" + 0.010*\"crowd\" + 0.008*\"cheering\" + 0.007*\"david\" + 0.006*\"von\" + 0.006*\"pam\" + 0.006*\"fritz\" + 0.005*\"erich\" + 0.005*\"friends\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"kev\" + 0.007*\"wee\" + 0.006*\"cos\" + 0.006*\"mate\" + 0.004*\"quite\" + 0.004*\"son\" + 0.004*\"telly\" + 0.003*\"everybody\" + 0.003*\"young\" + 0.003*\"bike\"'),\n",
       " (1,\n",
       "  '0.012*\"hey\" + 0.011*\"kerry\" + 0.010*\"crowd\" + 0.009*\"cheering\" + 0.007*\"david\" + 0.007*\"von\" + 0.006*\"pam\" + 0.006*\"fritz\" + 0.006*\"erich\" + 0.005*\"friends\"'),\n",
       " (2,\n",
       "  '0.016*\"blow\" + 0.014*\"penis\" + 0.007*\"sort\" + 0.007*\"teeth\" + 0.005*\"balls\" + 0.005*\"dick\" + 0.004*\"cock\" + 0.004*\"boyfriend\" + 0.003*\"bite\" + 0.003*\"rockhard\"'),\n",
       " (3,\n",
       "  '0.010*\"viktor\" + 0.010*\"sams\" + 0.005*\"sail\" + 0.005*\"rufus\" + 0.005*\"imogene\" + 0.004*\"detective\" + 0.004*\"collier\" + 0.004*\"llewellyn\" + 0.003*\"hm\" + 0.003*\"worlds\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "higher topivcs resulting in increased model complexity, also capture more nuances in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Death and other</th>\n",
       "      <td>death and other detailsseason  episode episode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacqueline</th>\n",
       "      <td>in get on your knees jacqueline novak transcen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kevin</th>\n",
       "      <td>kevin bridges the overdue catchup  is a comedy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taylor</th>\n",
       "      <td>in her  netflix standup comedy special have it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The iron claw</th>\n",
       "      <td>the iron claw directed by sean durkinwritten b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        transcript\n",
       "Death and other  death and other detailsseason  episode episode...\n",
       "Jacqueline       in get on your knees jacqueline novak transcen...\n",
       "Kevin            kevin bridges the overdue catchup  is a comedy...\n",
       "Taylor           in her  netflix standup comedy special have it...\n",
       "The iron claw    the iron claw directed by sean durkinwritten b..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('02D_data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Death and other</th>\n",
       "      <td>death detailsseason episode episode title rele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacqueline</th>\n",
       "      <td>get knees standup comedy show blend anecdotes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kevin</th>\n",
       "      <td>kevin catchup comedy bridges stage backdrop co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taylor</th>\n",
       "      <td>comedy taylor tomlinson exploration life relat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The iron claw</th>\n",
       "      <td>iron claw durkinwritten zac efron jeremy harri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        transcript\n",
       "Death and other  death detailsseason episode episode title rele...\n",
       "Jacqueline       get knees standup comedy show blend anecdotes ...\n",
       "Kevin            kevin catchup comedy bridges stage backdrop co...\n",
       "Taylor           comedy taylor tomlinson exploration life relat...\n",
       "The iron claw    iron claw durkinwritten zac efron jeremy harri..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>death</th>\n",
       "      <th>detailsseason</th>\n",
       "      <th>episode</th>\n",
       "      <th>title</th>\n",
       "      <th>release</th>\n",
       "      <th>date</th>\n",
       "      <th>hulu</th>\n",
       "      <th>details</th>\n",
       "      <th>glimpse</th>\n",
       "      <th>showcasing</th>\n",
       "      <th>...</th>\n",
       "      <th>clarence</th>\n",
       "      <th>messiah</th>\n",
       "      <th>debt</th>\n",
       "      <th>teenager</th>\n",
       "      <th>activist</th>\n",
       "      <th>cases</th>\n",
       "      <th>destination</th>\n",
       "      <th>sparks</th>\n",
       "      <th>playboy</th>\n",
       "      <th>exes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Death and other</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacqueline</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kevin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taylor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The iron claw</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3263 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 death  detailsseason  episode  title  release  date  hulu  \\\n",
       "Death and other      0              0        0      1        1     0     0   \n",
       "Jacqueline           1              3        1      0        1     0     1   \n",
       "Kevin                0              0        0      0        1     0     1   \n",
       "Taylor               0              0        0      0        0     0     2   \n",
       "The iron claw        0              0        0      0        1     3     0   \n",
       "\n",
       "                 details  glimpse  showcasing  ...  clarence  messiah  debt  \\\n",
       "Death and other        0        0           0  ...         0        0     3   \n",
       "Jacqueline             1        0           0  ...         1        0     1   \n",
       "Kevin                  1        2           0  ...        11        1     3   \n",
       "Taylor                 0        0           1  ...         1        0     4   \n",
       "The iron claw          0        0           0  ...         0        0     3   \n",
       "\n",
       "                 teenager  activist  cases  destination  sparks  playboy  exes  \n",
       "Death and other         0         0      0            0       0        1     0  \n",
       "Jacqueline              0         0      1            1       0        0     1  \n",
       "Kevin                   0         0      0            0       2        0     0  \n",
       "Taylor                  0         0      0            0       0        0     0  \n",
       "The iron claw           1         1      0            0       0        0     0  \n",
       "\n",
       "[5 rows x 3263 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people','youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "a_stop_words = list(text.ENGLISH_STOP_WORDS.union(add_stop_words))\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=a_stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.vocabulary_)\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"man\" + 0.008*\"kevin\" + 0.008*\"way\" + 0.006*\"world\" + 0.006*\"life\" + 0.006*\"hes\" + 0.005*\"gon\" + 0.005*\"door\" + 0.005*\"family\" + 0.005*\"bit\"'),\n",
       " (1,\n",
       "  '0.016*\"blow\" + 0.015*\"job\" + 0.014*\"penis\" + 0.014*\"way\" + 0.008*\"cause\" + 0.007*\"sort\" + 0.007*\"kind\" + 0.007*\"life\" + 0.006*\"thing\" + 0.006*\"teeth\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"kevin\" + 0.015*\"way\" + 0.011*\"von\" + 0.011*\"kerry\" + 0.010*\"crowd\" + 0.010*\"world\" + 0.009*\"hey\" + 0.009*\"champion\" + 0.009*\"man\" + 0.007*\"family\"'),\n",
       " (1,\n",
       "  '0.015*\"blow\" + 0.014*\"job\" + 0.013*\"penis\" + 0.013*\"way\" + 0.008*\"cause\" + 0.007*\"life\" + 0.007*\"kind\" + 0.007*\"sort\" + 0.006*\"thing\" + 0.005*\"teeth\"'),\n",
       " (2,\n",
       "  '0.009*\"man\" + 0.007*\"bit\" + 0.007*\"life\" + 0.006*\"school\" + 0.006*\"friends\" + 0.006*\"hes\" + 0.006*\"guy\" + 0.005*\"wife\" + 0.005*\"gon\" + 0.005*\"night\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"kevin\" + 0.013*\"way\" + 0.009*\"world\" + 0.009*\"von\" + 0.009*\"kerry\" + 0.009*\"hey\" + 0.009*\"man\" + 0.008*\"crowd\" + 0.008*\"champion\" + 0.007*\"family\"'),\n",
       " (1,\n",
       "  '0.012*\"friends\" + 0.008*\"theyre\" + 0.008*\"okay\" + 0.007*\"life\" + 0.007*\"lot\" + 0.006*\"friend\" + 0.006*\"parents\" + 0.006*\"man\" + 0.006*\"room\" + 0.006*\"talk\"'),\n",
       " (2,\n",
       "  '0.001*\"way\" + 0.001*\"job\" + 0.000*\"penis\" + 0.000*\"blow\" + 0.000*\"life\" + 0.000*\"kevin\" + 0.000*\"cause\" + 0.000*\"man\" + 0.000*\"world\" + 0.000*\"night\"'),\n",
       " (3,\n",
       "  '0.012*\"way\" + 0.012*\"blow\" + 0.011*\"job\" + 0.011*\"penis\" + 0.008*\"life\" + 0.008*\"bit\" + 0.007*\"man\" + 0.006*\"school\" + 0.006*\"cause\" + 0.006*\"kind\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Death and other</th>\n",
       "      <td>death other detailsseason episode episode titl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacqueline</th>\n",
       "      <td>get knees novak typical standup comedy show un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kevin</th>\n",
       "      <td>kevin overdue catchup comedy special kevin bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taylor</th>\n",
       "      <td>netflix standup comedy special taylor tomlinso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The iron claw</th>\n",
       "      <td>iron claw sean durkinwritten sean zac efron je...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        transcript\n",
       "Death and other  death other detailsseason episode episode titl...\n",
       "Jacqueline       get knees novak typical standup comedy show un...\n",
       "Kevin            kevin overdue catchup comedy special kevin bri...\n",
       "Taylor           netflix standup comedy special taylor tomlinso...\n",
       "The iron claw    iron claw sean durkinwritten sean zac efron je..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detailsseason</th>\n",
       "      <th>episode</th>\n",
       "      <th>title</th>\n",
       "      <th>tragicoriginal</th>\n",
       "      <th>release</th>\n",
       "      <th>date</th>\n",
       "      <th>february</th>\n",
       "      <th>hulu</th>\n",
       "      <th>sixth</th>\n",
       "      <th>details</th>\n",
       "      <th>...</th>\n",
       "      <th>teenager</th>\n",
       "      <th>unlikely</th>\n",
       "      <th>activist</th>\n",
       "      <th>landmark</th>\n",
       "      <th>cases</th>\n",
       "      <th>destination</th>\n",
       "      <th>sparks</th>\n",
       "      <th>jaded</th>\n",
       "      <th>playboy</th>\n",
       "      <th>exes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Death and other</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacqueline</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kevin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taylor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The iron claw</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 detailsseason  episode  title  tragicoriginal  release  date  \\\n",
       "Death and other              0        0      0               1        1     0   \n",
       "Jacqueline                   1        3      1               0        1     0   \n",
       "Kevin                        0        0      0               0        1     1   \n",
       "Taylor                       0        0      0               0        0     0   \n",
       "The iron claw                0        0      0               0        1     0   \n",
       "\n",
       "                 february  hulu  sixth  details  ...  teenager  unlikely  \\\n",
       "Death and other         0     0      0        0  ...         0         0   \n",
       "Jacqueline              0     1      1        0  ...         0         1   \n",
       "Kevin                   0     1      1        2  ...         1        11   \n",
       "Taylor                  0     2      0        0  ...         0         1   \n",
       "The iron claw           3     0      0        0  ...         0         0   \n",
       "\n",
       "                 activist  landmark  cases  destination  sparks  jaded  \\\n",
       "Death and other         0         0      0            0       0      0   \n",
       "Jacqueline              0         0      0            1       1      0   \n",
       "Kevin                   1         0      0            0       0      2   \n",
       "Taylor                  0         0      0            0       0      0   \n",
       "The iron claw           0         1      1            0       0      0   \n",
       "\n",
       "                 playboy  exes  \n",
       "Death and other        1     0  \n",
       "Jacqueline             0     2  \n",
       "Kevin                  0     0  \n",
       "Taylor                 0     0  \n",
       "The iron claw          0     0  \n",
       "\n",
       "[5 rows x 4002 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=a_stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.vocabulary_)\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"job\" + 0.010*\"blow\" + 0.010*\"penis\" + 0.007*\"cause\" + 0.005*\"theyre\" + 0.005*\"okay\" + 0.005*\"friends\" + 0.004*\"sort\" + 0.004*\"teeth\" + 0.004*\"single\"'),\n",
       " (1,\n",
       "  '0.010*\"kevin\" + 0.006*\"crowd\" + 0.006*\"hey\" + 0.005*\"kerry\" + 0.005*\"door\" + 0.005*\"kev\" + 0.005*\"von\" + 0.005*\"bit\" + 0.004*\"son\" + 0.004*\"champion\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"bit\" + 0.006*\"guy\" + 0.006*\"friends\" + 0.005*\"okay\" + 0.005*\"wee\" + 0.005*\"theyre\" + 0.005*\"kev\" + 0.005*\"wife\" + 0.004*\"door\" + 0.004*\"lot\"'),\n",
       " (1,\n",
       "  '0.019*\"kevin\" + 0.012*\"crowd\" + 0.011*\"kerry\" + 0.011*\"hey\" + 0.010*\"von\" + 0.008*\"champion\" + 0.008*\"david\" + 0.007*\"erich\" + 0.007*\"pam\" + 0.007*\"fritz\"'),\n",
       " (2,\n",
       "  '0.014*\"blow\" + 0.013*\"job\" + 0.013*\"penis\" + 0.008*\"cause\" + 0.006*\"sort\" + 0.005*\"teeth\" + 0.004*\"balls\" + 0.004*\"viktor\" + 0.004*\"dick\" + 0.004*\"sams\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"kevin\" + 0.008*\"crowd\" + 0.007*\"kerry\" + 0.007*\"door\" + 0.007*\"hey\" + 0.007*\"kev\" + 0.006*\"von\" + 0.006*\"bit\" + 0.006*\"son\" + 0.005*\"champion\"'),\n",
       " (1,\n",
       "  '0.011*\"friends\" + 0.010*\"okay\" + 0.008*\"theyre\" + 0.007*\"single\" + 0.006*\"lot\" + 0.006*\"parents\" + 0.005*\"talk\" + 0.005*\"married\" + 0.004*\"job\" + 0.004*\"shes\"'),\n",
       " (2,\n",
       "  '0.015*\"blow\" + 0.014*\"job\" + 0.013*\"penis\" + 0.008*\"cause\" + 0.006*\"sort\" + 0.006*\"teeth\" + 0.004*\"viktor\" + 0.004*\"balls\" + 0.004*\"dick\" + 0.004*\"sams\"'),\n",
       " (3,\n",
       "  '0.000*\"kevin\" + 0.000*\"penis\" + 0.000*\"job\" + 0.000*\"crowd\" + 0.000*\"kerry\" + 0.000*\"blow\" + 0.000*\"door\" + 0.000*\"okay\" + 0.000*\"bit\" + 0.000*\"cause\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"kevin\" + 0.013*\"crowd\" + 0.013*\"kerry\" + 0.012*\"hey\" + 0.011*\"von\" + 0.009*\"champion\" + 0.009*\"david\" + 0.008*\"erich\" + 0.008*\"pam\" + 0.007*\"fritz\"'),\n",
       " (1,\n",
       "  '0.008*\"bit\" + 0.007*\"wee\" + 0.007*\"wife\" + 0.006*\"kev\" + 0.006*\"door\" + 0.005*\"cos\" + 0.005*\"viktor\" + 0.005*\"guy\" + 0.004*\"sams\" + 0.004*\"old\"'),\n",
       " (2,\n",
       "  '0.000*\"mistakes\" + 0.000*\"job\" + 0.000*\"huh\" + 0.000*\"dream\" + 0.000*\"whatd\" + 0.000*\"okay\" + 0.000*\"secrets\" + 0.000*\"kinda\" + 0.000*\"farm\" + 0.000*\"offer\"'),\n",
       " (3,\n",
       "  '0.013*\"job\" + 0.012*\"blow\" + 0.011*\"penis\" + 0.008*\"cause\" + 0.006*\"okay\" + 0.006*\"theyre\" + 0.006*\"friends\" + 0.005*\"sort\" + 0.005*\"teeth\" + 0.004*\"single\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "#increasing number of passes elads to better model converfgence\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el278362309079767904219083250\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el278362309079767904219083250_data = {\"mdsDat\": {\"x\": [0.08024118896883446, 0.08465884037253886, -0.17418783275417127, 0.009287803412797951], \"y\": [0.1382668059293973, -0.11684185160836993, -0.028087929514939988, 0.006662975193912635], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [45.99598636679949, 32.26413172961706, 21.72879080684325, 0.011091096740197164]}, \"tinfo\": {\"Term\": [\"kevin\", \"crowd\", \"penis\", \"kerry\", \"von\", \"blow\", \"hey\", \"job\", \"champion\", \"david\", \"erich\", \"pam\", \"wee\", \"fritz\", \"grunts\", \"wife\", \"bit\", \"brother\", \"sort\", \"friends\", \"race\", \"heavyweight\", \"kev\", \"teeth\", \"door\", \"viktor\", \"cos\", \"mike\", \"indistinct\", \"nwa\", \"penis\", \"sort\", \"teeth\", \"dick\", \"boyfriend\", \"blow\", \"dignity\", \"ghost\", \"vulva\", \"breakup\", \"college\", \"friends\", \"jobs\", \"toothy\", \"afraid\", \"queen\", \"hugh\", \"rockhard\", \"spirit\", \"freeze\", \"bone\", \"tip\", \"ones\", \"netflix\", \"parents\", \"sensitive\", \"boner\", \"authentic\", \"risk\", \"sexual\", \"job\", \"balls\", \"single\", \"cock\", \"cause\", \"weird\", \"married\", \"theyre\", \"shes\", \"dream\", \"year\", \"person\", \"okay\", \"problem\", \"lot\", \"talk\", \"date\", \"ta\", \"guy\", \"body\", \"course\", \"idea\", \"bit\", \"wee\", \"cos\", \"viktor\", \"sams\", \"mate\", \"innit\", \"cork\", \"glasgow\", \"laugh\", \"covid\", \"collier\", \"generation\", \"sunday\", \"detective\", \"boots\", \"cillian\", \"sail\", \"factory\", \"hm\", \"imogene\", \"swimmer\", \"instagram\", \"husband\", \"ya\", \"governor\", \"rufus\", \"wife\", \"silence\", \"sign\", \"telly\", \"bike\", \"cunt\", \"everybody\", \"bit\", \"kev\", \"evening\", \"youth\", \"gym\", \"door\", \"health\", \"days\", \"old\", \"guy\", \"young\", \"game\", \"son\", \"doctor\", \"mental\", \"minute\", \"special\", \"dog\", \"von\", \"champion\", \"david\", \"erich\", \"pam\", \"fritz\", \"grunts\", \"race\", \"heavyweight\", \"mike\", \"indistinct\", \"nwa\", \"chuckles\", \"harley\", \"crowd\", \"referee\", \"ring\", \"kerry\", \"texas\", \"gino\", \"doris\", \"sighs\", \"match\", \"groans\", \"pop\", \"announcer\", \"chatter\", \"iron\", \"champ\", \"oldest\", \"ric\", \"kevin\", \"brothers\", \"brother\", \"hey\", \"flair\", \"title\", \"thanks\", \"son\", \"wan\", \"door\", \"okay\", \"uh\", \"durkinwritten\", \"zac\", \"efron\", \"jeremy\", \"harris\", \"dickinson\", \"maura\", \"tierney\", \"holt\", \"mccallany\", \"kingdomrelease\", \"december\", \"kingdomrunning\", \"crossroads\", \"ambition\", \"youngest\", \"aspirations\", \"chronicles\", \"unfolding\", \"misfortune\", \"key\", \"kevins\", \"battles\", \"grief\", \"toll\", \"welcomes\", \"beginnings\", \"solace\", \"hawaii\", \"wwe\", \"job\", \"mistakes\", \"huh\", \"dream\", \"okay\", \"whatd\", \"secrets\", \"kinda\", \"farm\", \"offer\", \"career\", \"date\", \"favorite\", \"girl\", \"hand\", \"tonight\", \"open\", \"wan\", \"hey\", \"ass\", \"shot\", \"na\", \"soft\", \"sad\", \"bad\", \"lips\", \"wouldnt\", \"wont\", \"friends\", \"questions\"], \"Freq\": [59.0, 34.0, 58.0, 31.0, 28.0, 64.0, 37.0, 71.0, 23.0, 21.0, 20.0, 19.0, 24.0, 18.0, 16.0, 25.0, 42.0, 18.0, 27.0, 32.0, 14.0, 14.0, 31.0, 24.0, 34.0, 17.0, 17.0, 13.0, 13.0, 12.0, 58.077900866568115, 27.046774061892442, 24.53073623032949, 19.498661776816224, 16.143945071269833, 63.11021845222147, 11.111865779306019, 11.111865779306019, 11.111865779306019, 11.111865779306019, 10.27318327648467, 31.240721224784863, 8.595825528517793, 8.595824923711474, 8.595825528517793, 7.757143025696443, 7.757143025696443, 6.918461732487731, 6.918461732487731, 6.918461732487731, 6.918461732487731, 6.918461732487731, 6.918461732487731, 6.918461732487731, 19.498763384277755, 6.079780439279018, 6.079780439279018, 6.079780439279018, 6.079780439279018, 6.079780439279018, 67.30465671784462, 18.660079671843768, 22.85347767059887, 14.466662319286469, 41.30507334673445, 12.789290055967944, 11.95060513392132, 32.91774753888913, 15.30548997562429, 13.628187869838728, 16.14399224616269, 16.982737648841177, 32.917933819235266, 15.30535691823419, 19.498725886286, 12.789224736885533, 11.950757545113616, 15.305225070456729, 16.982582818423605, 13.627959253050285, 11.950743029761968, 11.950740610536695, 12.789034827701482, 24.52101783846873, 17.458836205772794, 17.45883281181566, 15.889460246980054, 13.53540139972664, 9.611965745191204, 8.827277765794834, 8.042590634887747, 7.257902655491377, 7.257902655491377, 7.257901807002095, 6.473215100339649, 6.473215100339649, 6.473213403361084, 5.68852712094328, 5.68852712094328, 5.688524999720073, 5.688524999720073, 5.688524999720073, 5.688524999720073, 4.903837444568345, 4.903837444568345, 4.903837444568345, 4.903836596079063, 4.903836596079063, 4.903836596079063, 23.73678804328491, 4.119146919704128, 4.119146919704128, 4.119146919704128, 9.612093867072863, 9.612093867072863, 11.966239260233092, 30.013998301164122, 22.16698105193666, 8.827440675737074, 8.82735752378739, 8.82733716004461, 20.59784097316446, 8.042661059498196, 10.396732634090846, 15.104904631911754, 16.674307742361535, 11.1814358862943, 11.181434189315736, 13.535417521023007, 8.827360069255237, 8.042641544244697, 8.042656817051782, 8.042664453455325, 7.258012110608821, 27.888515692209545, 22.91471699187168, 21.493633608101614, 20.072545652903123, 18.651458840561737, 17.940917148676704, 15.809286358736074, 14.388200689251796, 14.388200689251796, 12.967113876910412, 12.967113876910412, 12.256568756454058, 12.256568756454058, 11.54602592171192, 32.86264124967996, 10.83548308696978, 10.83548308696978, 30.73099560259694, 9.414395703199842, 8.703853439886258, 7.993309462287011, 7.993309462287011, 7.282766627544873, 7.282766627544873, 6.572220935659967, 5.861677529489274, 5.151134123318582, 4.440588145719399, 4.440588145719399, 4.440588145719399, 11.546136778851283, 51.33751609580872, 10.125125395078948, 15.809530930156935, 29.310780790228065, 8.703934582740843, 8.704189439875664, 10.835763086960954, 12.967254448334552, 10.125337966500817, 12.967372162616554, 14.388661260665847, 10.125289966502331, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.0003130278678869564, 0.00031303060235056895, 0.00031303060235056895, 0.0003130305658910541, 0.0003130304929720245, 0.0003130304929720245, 0.0003130304929720245, 0.0003130304565125096, 0.0003130304565125096, 0.0003130304565125096, 0.00031303038359347995, 0.00031303038359347995, 0.0003130303106744503, 0.0003130303106744503, 0.0003130303106744503, 0.0003130303106744503, 0.00031303027421493543, 0.00031303027421493543, 0.00031303027421493543, 0.0003130302377554206, 0.0003130302377554206, 0.0003130302377554206, 0.0003130302377554206, 0.0003130302012959058, 0.0003130302012959058, 0.0003130302012959058, 0.0003130302012959058, 0.0003130302012959058, 0.0003130302012959058, 0.00031303016483639096, 0.00031303016483639096], \"Total\": [59.0, 34.0, 58.0, 31.0, 28.0, 64.0, 37.0, 71.0, 23.0, 21.0, 20.0, 19.0, 24.0, 18.0, 16.0, 25.0, 42.0, 18.0, 27.0, 32.0, 14.0, 14.0, 31.0, 24.0, 34.0, 17.0, 17.0, 13.0, 13.0, 12.0, 58.45202218342247, 27.42089503198798, 24.90485720038857, 19.87278267541027, 16.51806596986388, 64.90488741253576, 11.48598650435961, 11.485986517653714, 11.485986544132546, 11.485986735208055, 10.647304041420576, 32.398469457767725, 8.969946253644304, 8.969945648765066, 8.96994629341724, 8.131263732892892, 8.131263927990593, 7.292582395311747, 7.292582421790577, 7.292582421827037, 7.292582439684179, 7.292582524011845, 7.292582550563594, 7.292582643980875, 20.65692832707478, 6.453901102066575, 6.453901102066575, 6.453901102103035, 6.453901128581864, 6.453901128618324, 71.45193965377895, 19.818246965624702, 24.796386717039674, 15.624852044389046, 46.237540441181686, 13.947491147727034, 13.108811404856581, 39.34596095575855, 17.248281137546517, 15.496361907448446, 19.656351546515886, 21.279727763071623, 49.071074193178205, 20.164292003877094, 28.207005231257945, 16.005206628524746, 14.603738861253877, 22.78450930613296, 35.25548425393649, 20.766865781684796, 16.024868216140778, 16.09902104788853, 42.98098252722708, 24.908636985867894, 17.846455209235305, 17.846452057647085, 16.277079491804884, 13.923020310701352, 9.999584601551435, 9.21489662316166, 8.430209492254573, 7.645521476100865, 7.6455214771439195, 7.645520886831044, 6.8608339030919945, 6.860833957925233, 6.860832501047176, 6.0761459236591655, 6.076145923695625, 6.076144079549022, 6.0761440973697045, 6.076144097406164, 6.076144097406164, 5.291456228384033, 5.291456247284231, 5.291456284187408, 5.291455639004834, 5.291455674864957, 5.291455675908011, 25.67201139568546, 4.506765685662674, 4.506765685662674, 4.506765704562871, 10.837476874143913, 10.83747746112955, 14.61229242202278, 42.98098252722708, 31.791341041881946, 10.762968276511398, 10.891550745115039, 10.891553461460223, 34.612514094753536, 10.106862445711148, 15.430944397731235, 28.269633767073692, 35.25548425393649, 18.85982554851425, 20.537191114927285, 32.58257915736149, 15.538945135929835, 12.622949861069584, 13.20502312308306, 17.142174415555814, 10.871058980578521, 28.294670599579078, 23.320871906819846, 21.899788424603717, 20.47870043164129, 19.057613619263446, 18.347071914157222, 16.21544110894995, 14.79435543579566, 14.794355454695857, 13.373268596938987, 13.37326865363958, 12.662723431067144, 12.662723457582436, 11.95218066064069, 34.0525769389494, 11.241637780483064, 11.241637806998355, 31.920955000205723, 9.820550370161376, 9.110008133399543, 8.399464097090704, 8.399464129248546, 7.688921281248762, 7.6889213021215, 6.978375570463658, 6.267832164292965, 5.557288758122274, 4.846742754007801, 4.846742754007801, 4.846742754007801, 12.736310162668238, 59.69612215062461, 11.368963864769007, 18.730758904017762, 37.93930724640192, 9.89413967884473, 10.732004763975066, 16.841224583842067, 32.58257915736149, 16.346619708738636, 34.612514094753536, 49.071074193178205, 20.540038399607944, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 1.2939655160563257, 71.45193965377895, 2.9160031431237945, 3.626578737745701, 15.496361907448446, 49.071074193178205, 3.7547627818249945, 3.7007448875024678, 3.7007448875024678, 5.304045930330954, 4.4113206790665265, 6.142740553990452, 14.603738861253877, 5.304046209278998, 13.690894712571065, 11.500980707439945, 13.504456892533325, 6.981429532302789, 16.346619708738636, 37.93930724640192, 4.593471229416959, 7.307520809305375, 15.892365051291765, 2.91600441114137, 8.65879951024548, 12.41386429139395, 2.91600441114137, 5.432165086250924, 2.91600441114137, 32.398469457767725, 10.35635250239563], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.4937, -5.2579, -5.3556, -5.5852, -5.774, -4.4106, -6.1475, -6.1475, -6.1475, -6.1475, -6.226, -5.1138, -6.4042, -6.4042, -6.4042, -6.5069, -6.5069, -6.6213, -6.6213, -6.6213, -6.6213, -6.6213, -6.6213, -6.6213, -5.5851, -6.7505, -6.7505, -6.7505, -6.7505, -6.7505, -4.3463, -5.6291, -5.4264, -5.8837, -4.8345, -6.0069, -6.0747, -5.0615, -5.8273, -5.9434, -5.774, -5.7233, -5.0615, -5.8273, -5.5852, -6.0069, -6.0747, -5.8273, -5.7233, -5.9434, -6.0747, -6.0747, -6.0069, -5.0014, -5.3411, -5.3411, -5.4352, -5.5956, -5.9379, -6.0231, -6.1162, -6.2188, -6.2188, -6.2188, -6.3332, -6.3332, -6.3332, -6.4625, -6.4625, -6.4625, -6.4625, -6.4625, -6.4625, -6.6109, -6.6109, -6.6109, -6.6109, -6.6109, -6.6109, -5.0339, -6.7853, -6.7853, -6.7853, -5.9379, -5.9379, -5.7188, -4.7992, -5.1023, -6.023, -6.023, -6.023, -5.1757, -6.1161, -5.8594, -5.4859, -5.387, -5.7866, -5.7866, -5.5956, -6.023, -6.1161, -6.1161, -6.1161, -6.2188, -4.4774, -4.6738, -4.7378, -4.8062, -4.8797, -4.9185, -5.045, -5.1392, -5.1392, -5.2432, -5.2432, -5.2995, -5.2995, -5.3592, -4.3132, -5.4228, -5.4228, -4.3803, -5.5633, -5.6418, -5.727, -5.727, -5.8201, -5.8201, -5.9227, -6.0371, -6.1664, -6.3148, -6.3148, -6.3148, -5.3592, -3.8672, -5.4906, -5.045, -4.4276, -5.6418, -5.6418, -5.4227, -5.2432, -5.4905, -5.2431, -5.1391, -5.4905, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2946, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945, -8.2945], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7702, 0.7629, 0.7615, 0.7576, 0.7537, 0.7486, 0.7435, 0.7435, 0.7435, 0.7435, 0.7408, 0.7402, 0.734, 0.734, 0.734, 0.7295, 0.7295, 0.724, 0.724, 0.724, 0.724, 0.724, 0.724, 0.724, 0.7189, 0.7169, 0.7169, 0.7169, 0.7169, 0.7169, 0.7168, 0.7164, 0.695, 0.6996, 0.6638, 0.6899, 0.6841, 0.5982, 0.6571, 0.6482, 0.5798, 0.5511, 0.3774, 0.5009, 0.4074, 0.5523, 0.5761, 0.3787, 0.0462, 0.3554, 0.4833, 0.4787, -0.4356, 1.1155, 1.1093, 1.1093, 1.1071, 1.103, 1.0917, 1.0882, 1.0841, 1.0792, 1.0792, 1.0792, 1.0731, 1.0731, 1.0731, 1.0653, 1.0653, 1.0653, 1.0653, 1.0653, 1.0653, 1.0551, 1.0551, 1.0551, 1.0551, 1.0551, 1.0551, 1.0528, 1.0413, 1.0413, 1.0413, 1.0112, 1.0112, 0.9314, 0.7721, 0.7706, 0.933, 0.9211, 0.9211, 0.6122, 0.9028, 0.7363, 0.5044, 0.3825, 0.6084, 0.5232, 0.2527, 0.5657, 0.6805, 0.6354, 0.3744, 0.7272, 1.5121, 1.509, 1.5078, 1.5065, 1.505, 1.5041, 1.5012, 1.4987, 1.4987, 1.4957, 1.4957, 1.4939, 1.4939, 1.492, 1.491, 1.4897, 1.4897, 1.4885, 1.4843, 1.4809, 1.477, 1.477, 1.4723, 1.4723, 1.4666, 1.4595, 1.4506, 1.439, 1.439, 1.439, 1.4284, 1.3757, 1.4107, 1.357, 1.2685, 1.3984, 1.3171, 1.0856, 0.6052, 1.0476, 0.5448, 0.2997, 0.8192, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, 0.7799, -3.2315, -0.0326, -0.2507, -1.703, -2.8557, -0.2855, -0.271, -0.271, -0.6309, -0.4466, -0.7777, -1.6437, -0.6309, -1.5792, -1.4049, -1.5654, -0.9057, -1.7564, -2.5984, -0.4871, -0.9513, -1.7283, -0.0326, -1.121, -1.4812, -0.0326, -0.6548, -0.0326, -2.4405, -1.3]}, \"token.table\": {\"Topic\": [1, 3, 3, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 3, 1, 2, 1, 2, 1, 3, 1, 2, 3, 1, 1, 2, 1, 1, 1, 3, 1, 3, 1, 2, 3, 1, 2, 3, 3, 3, 3, 3, 3, 2, 1, 2, 1, 2, 2, 2, 1, 2, 3, 2, 3, 2, 3, 1, 2, 1, 2, 3, 3, 1, 2, 3, 3, 2, 1, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 3, 3, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 2, 3, 1, 1, 2, 3, 1, 2, 3, 2, 1, 3, 1, 2, 3, 2, 2, 3, 3, 3, 1, 2, 3, 1, 2, 1, 2, 3, 3, 3, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 1, 2, 3, 2, 1, 2, 3, 2, 3, 2, 2, 3, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 3, 3, 1, 2, 3, 3, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 1, 2, 3, 3, 1, 2, 1, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 3, 3, 2, 3, 3, 1, 1, 2, 1, 2, 3, 2, 2, 1, 2, 3, 1, 1, 1, 2, 1, 2, 3, 3, 2, 2, 1, 2, 1, 2, 3, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 2, 1, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 1, 2, 3, 3, 1, 2, 3, 1, 1, 2, 3, 3, 2, 3, 1, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 2, 1, 2, 1, 2, 3, 3, 1, 2, 3], \"Freq\": [1.0033504890218592, 0.772818121960269, 0.9572687721571789, 0.772818121960269, 0.653100857753883, 0.21770028591796103, 0.21770028591796103, 0.929670273076368, 0.5638856552389435, 0.16111018721112672, 0.32222037442225343, 0.9587124447967585, 0.050458549726145184, 0.772818121960269, 0.772818121960269, 0.0922723998964928, 0.9227239989649281, 0.3024593491264402, 0.6979831133687081, 0.970651094417154, 0.030814320457687432, 0.674150839475604, 0.19261452556445827, 0.1444608941733437, 0.959879447081458, 0.9296702730816199, 0.9874680554720271, 0.9686364026630565, 0.9576887257131895, 0.1601643593499299, 0.8542099165329595, 0.08795876316388643, 0.8795876316388642, 0.6511751497304435, 0.1627937874326109, 0.3255875748652218, 0.8867253666348385, 0.06488234390011013, 0.043254895933406756, 0.825296534810389, 0.986240998702711, 0.8997193087532537, 0.772818121960269, 0.9476634343470877, 0.9874680554661018, 0.8960084844468952, 0.06400060603192108, 0.9392048880258883, 0.9155687498097199, 0.976679432016468, 0.9525701211074524, 0.7488361113580455, 0.12480601855967426, 0.12480601855967426, 0.9155686791183978, 0.772818121960269, 0.029366353148333926, 0.9690896538950196, 0.09227239489878244, 0.9227239489878244, 0.8217073801448186, 0.13695123002413642, 0.06847561501206821, 0.958913373629088, 0.19441454279629708, 0.648048475987657, 0.1296096951975314, 0.772818121960269, 0.8745294392603542, 0.9560815065677635, 0.772818121960269, 0.9576887449610749, 0.2574177310627749, 0.5791898948912435, 0.12870886553138744, 0.27596207557696006, 0.6439115096795734, 0.09198735852565335, 0.028891284731945466, 0.6067169793708548, 0.37558670151529105, 0.9524417162246024, 0.9034378574541934, 0.06453127553244238, 0.06453127553244238, 0.772818121960269, 0.772818121960269, 0.9766244721807806, 0.0929111723001501, 0.8362005507013509, 0.0929111723001501, 0.06843553161397588, 0.8212263793677105, 0.13687106322795176, 0.9874683522725101, 0.5656059618271085, 0.1885353206090362, 0.3770706412180724, 0.5656059320810108, 0.18853531069367027, 0.37707062138734054, 0.10106992951981077, 0.9096293656782969, 0.959879449431888, 0.9568353233602388, 0.030865655592265766, 0.981082980664102, 0.3895372037603169, 0.5356136551704358, 0.09738430094007923, 0.8745292605460045, 0.9576887438526275, 0.9879244747327693, 0.8034536990412855, 0.0730412453673896, 0.1460824907347792, 0.9489681137046669, 0.9449195660374884, 0.772818121960269, 0.9104007863975125, 0.9867138298919886, 0.48219448292223777, 0.48219448292223777, 0.05672876269673385, 0.18362853444891977, 0.826328405020139, 0.5216946408855917, 0.08694910681426528, 0.3477964272570611, 1.004000888266087, 0.772818121960269, 0.772818121960269, 0.1978853487660457, 0.7915413950641828, 0.9463068562109124, 0.1581473262290267, 0.07907366311451335, 0.7643787434402957, 0.9874683522665848, 0.772818121960269, 0.9838568850853878, 0.2757419795114127, 0.2757419795114127, 0.5514839590228254, 0.9449194572279895, 0.7453869377712169, 0.18634673444280422, 0.12423115629520282, 0.9874683522665848, 0.9720884502280606, 1.0000415415704869, 0.9449194638179581, 0.825296534810389, 0.772818121960269, 0.9376932288283444, 0.041986263977388555, 0.02799084265159237, 1.003350493470737, 0.03132738353202638, 0.9711488894928179, 0.6920123303706244, 0.28309595333343723, 0.03350301372932771, 0.10050904118798311, 0.8543268500978565, 0.772818121960269, 0.772818121960269, 0.27021587015550075, 0.5404317403110015, 0.27021587015550075, 0.772818121960269, 0.772818121960269, 0.915568679243306, 0.3429350093502034, 0.3429350093502034, 0.3429350093502034, 0.6735915367202794, 0.2127131168590356, 0.1063565584295178, 0.9154148022569165, 0.07628456685474305, 0.910400788868933, 1.0055289504418434, 0.772818121960269, 0.772818121960269, 0.31688314094761577, 0.6337662818952315, 0.9720884543495654, 0.30291503185691465, 0.6058300637138293, 0.15145751592845733, 0.772818121960269, 0.34293515847474054, 0.34293515847474054, 0.34293515847474054, 0.440463076289015, 0.06292329661271642, 0.5033863729017314, 0.9598794201910943, 0.9476634363314611, 0.22668948207401884, 0.4533789641480377, 0.4533789641480377, 0.672493939506782, 0.04075720845495649, 0.28530045918469543, 0.3183628084521743, 0.5306046807536238, 0.14149458153429967, 0.825296534810389, 0.9598794324870573, 0.5729485603904134, 0.14323714009760335, 0.2864742801952067, 0.996976871269695, 0.9197882521137924, 0.04840990800598908, 0.9922667827983089, 0.7988824006245715, 0.1879723295587227, 1.0030987769743818, 0.7438892472453718, 0.14877784944907435, 0.09918523296604957, 0.9838569086915852, 0.6759136480127305, 0.2896772777197416, 0.0965590925732472, 0.9463068574198454, 0.9785051088460991, 0.07851567582981205, 0.9421881099577445, 0.9785051065381304, 0.9296702692621507, 0.9598794529219385, 0.9449195658512253, 0.692936704782289, 0.11548945079704816, 0.23097890159409631, 0.9874683551686494, 0.9829773214571824, 0.27021587015550075, 0.5404317403110015, 0.27021587015550075, 0.9296702730816199, 0.9296702692568988, 0.8696518731566591, 0.11595358308755456, 0.27369063355292894, 0.13684531677646447, 0.5473812671058579, 0.9524417125781233, 0.887554463442632, 0.887554463442632, 0.9275544966474802, 0.0806569127519548, 0.3429350093502034, 0.3429350093502034, 0.3429350093502034, 0.772818121960269, 0.18414748479616294, 0.4296774645243802, 0.398986217058353, 0.9846505728023472, 0.35001393957088944, 0.46668525276118594, 0.17500696978544472, 0.9598794494366869, 0.8745292535565814, 0.9449194671930526, 0.6583420252093125, 0.30722627843101247, 0.8122356869064835, 0.18743900467072697, 1.0038202507585527, 0.8875544597204607, 0.9164455820465496, 0.11875620980191992, 0.23751241960383984, 0.6531591539105596, 0.8387137891257991, 0.10166227746979382, 0.05083113873489691, 0.772818121960269, 0.9598794359819068, 0.09317923556619875, 0.09317923556619875, 0.8386131200957888, 0.772818121960269, 0.3702481365810811, 0.07404962731621621, 0.5183473912135135, 1.0033505611306655, 0.43816860635332533, 0.04868540070592504, 0.48685400705925036, 0.772818121960269, 0.9525702893262538, 0.9895856501123762, 0.957688741644852, 0.3058736355949531, 0.06117472711899062, 0.6117472711899062, 1.003667925072895, 0.9320672701856174, 0.07169748232197057, 0.772818121960269, 0.5326568191420882, 0.2663284095710441, 0.2663284095710441, 0.038952927551600576, 0.9348702612384138, 0.038952927551600576, 0.3429350093502034, 0.3429350093502034, 0.3429350093502034, 0.7363546461657058, 0.18408866154142645, 0.18408866154142645, 0.772818121960269, 0.9449195724411954, 0.8139862558998656, 0.1526224229812248, 0.31813655882265945, 0.5832503578415423, 0.10604551960755315, 0.772818121960269, 0.18362858024574863, 0.8263286111058687, 0.772818121960269], \"Term\": [\"afraid\", \"ambition\", \"announcer\", \"aspirations\", \"ass\", \"ass\", \"ass\", \"authentic\", \"bad\", \"bad\", \"bad\", \"balls\", \"balls\", \"battles\", \"beginnings\", \"bike\", \"bike\", \"bit\", \"bit\", \"blow\", \"blow\", \"body\", \"body\", \"body\", \"bone\", \"boner\", \"boots\", \"boyfriend\", \"breakup\", \"brother\", \"brother\", \"brothers\", \"brothers\", \"career\", \"career\", \"career\", \"cause\", \"cause\", \"cause\", \"champ\", \"champion\", \"chatter\", \"chronicles\", \"chuckles\", \"cillian\", \"cock\", \"cock\", \"college\", \"collier\", \"cork\", \"cos\", \"course\", \"course\", \"course\", \"covid\", \"crossroads\", \"crowd\", \"crowd\", \"cunt\", \"cunt\", \"date\", \"date\", \"date\", \"david\", \"days\", \"days\", \"days\", \"december\", \"detective\", \"dick\", \"dickinson\", \"dignity\", \"doctor\", \"doctor\", \"doctor\", \"dog\", \"dog\", \"dog\", \"door\", \"door\", \"door\", \"doris\", \"dream\", \"dream\", \"dream\", \"durkinwritten\", \"efron\", \"erich\", \"evening\", \"evening\", \"evening\", \"everybody\", \"everybody\", \"everybody\", \"factory\", \"farm\", \"farm\", \"farm\", \"favorite\", \"favorite\", \"favorite\", \"flair\", \"flair\", \"freeze\", \"friends\", \"friends\", \"fritz\", \"game\", \"game\", \"game\", \"generation\", \"ghost\", \"gino\", \"girl\", \"girl\", \"girl\", \"glasgow\", \"governor\", \"grief\", \"groans\", \"grunts\", \"guy\", \"guy\", \"guy\", \"gym\", \"gym\", \"hand\", \"hand\", \"hand\", \"harley\", \"harris\", \"hawaii\", \"health\", \"health\", \"heavyweight\", \"hey\", \"hey\", \"hey\", \"hm\", \"holt\", \"hugh\", \"huh\", \"huh\", \"huh\", \"husband\", \"idea\", \"idea\", \"idea\", \"imogene\", \"indistinct\", \"innit\", \"instagram\", \"iron\", \"jeremy\", \"job\", \"job\", \"job\", \"jobs\", \"kerry\", \"kerry\", \"kev\", \"kev\", \"kevin\", \"kevin\", \"kevin\", \"kevins\", \"key\", \"kinda\", \"kinda\", \"kinda\", \"kingdomrelease\", \"kingdomrunning\", \"laugh\", \"lips\", \"lips\", \"lips\", \"lot\", \"lot\", \"lot\", \"married\", \"married\", \"match\", \"mate\", \"maura\", \"mccallany\", \"mental\", \"mental\", \"mike\", \"minute\", \"minute\", \"minute\", \"misfortune\", \"mistakes\", \"mistakes\", \"mistakes\", \"na\", \"na\", \"na\", \"netflix\", \"nwa\", \"offer\", \"offer\", \"offer\", \"okay\", \"okay\", \"okay\", \"old\", \"old\", \"old\", \"oldest\", \"ones\", \"open\", \"open\", \"open\", \"pam\", \"parents\", \"parents\", \"penis\", \"person\", \"person\", \"pop\", \"problem\", \"problem\", \"problem\", \"queen\", \"questions\", \"questions\", \"questions\", \"race\", \"referee\", \"ric\", \"ric\", \"ring\", \"risk\", \"rockhard\", \"rufus\", \"sad\", \"sad\", \"sad\", \"sail\", \"sams\", \"secrets\", \"secrets\", \"secrets\", \"sensitive\", \"sexual\", \"shes\", \"shes\", \"shot\", \"shot\", \"shot\", \"sighs\", \"sign\", \"silence\", \"single\", \"single\", \"soft\", \"soft\", \"soft\", \"solace\", \"son\", \"son\", \"son\", \"sort\", \"special\", \"special\", \"special\", \"spirit\", \"sunday\", \"swimmer\", \"ta\", \"ta\", \"talk\", \"talk\", \"teeth\", \"telly\", \"texas\", \"thanks\", \"thanks\", \"thanks\", \"theyre\", \"theyre\", \"theyre\", \"tierney\", \"tip\", \"title\", \"title\", \"title\", \"toll\", \"tonight\", \"tonight\", \"tonight\", \"toothy\", \"uh\", \"uh\", \"uh\", \"unfolding\", \"viktor\", \"von\", \"vulva\", \"wan\", \"wan\", \"wan\", \"wee\", \"weird\", \"weird\", \"welcomes\", \"whatd\", \"whatd\", \"whatd\", \"wife\", \"wife\", \"wife\", \"wont\", \"wont\", \"wont\", \"wouldnt\", \"wouldnt\", \"wouldnt\", \"wwe\", \"ya\", \"year\", \"year\", \"young\", \"young\", \"young\", \"youngest\", \"youth\", \"youth\", \"zac\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 2, 1, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el278362309079767904219083250\", ldavis_el278362309079767904219083250_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el278362309079767904219083250\", ldavis_el278362309079767904219083250_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el278362309079767904219083250\", ldavis_el278362309079767904219083250_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "3      0.080241  0.138267       1        1  45.995986\n",
       "1      0.084659 -0.116842       2        1  32.264132\n",
       "0     -0.174188 -0.028088       3        1  21.728791\n",
       "2      0.009288  0.006663       4        1   0.011091, topic_info=           Term       Freq      Total Category  logprob  loglift\n",
       "1963      kevin  59.000000  59.000000  Default  30.0000  30.0000\n",
       "2957      crowd  34.000000  34.000000  Default  29.0000  29.0000\n",
       "829       penis  58.000000  58.000000  Default  28.0000  28.0000\n",
       "3007      kerry  31.000000  31.000000  Default  27.0000  27.0000\n",
       "3558        von  28.000000  28.000000  Default  26.0000  26.0000\n",
       "...         ...        ...        ...      ...      ...      ...\n",
       "413        lips   0.000313   2.916004   Topic4  -8.2945  -0.0326\n",
       "488     wouldnt   0.000313   5.432165   Topic4  -8.2945  -0.6548\n",
       "571        wont   0.000313   2.916004   Topic4  -8.2945  -0.0326\n",
       "212     friends   0.000313  32.398469   Topic4  -8.2945  -2.4405\n",
       "263   questions   0.000313  10.356353   Topic4  -8.2945  -1.3000\n",
       "\n",
       "[237 rows x 6 columns], token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "1181      1  1.003350       afraid\n",
       "3561      3  0.772818     ambition\n",
       "3618      3  0.957269    announcer\n",
       "3568      3  0.772818  aspirations\n",
       "323       1  0.653101          ass\n",
       "...     ...       ...          ...\n",
       "715       3  0.106046        young\n",
       "3565      3  0.772818     youngest\n",
       "1378      1  0.183629        youth\n",
       "1378      2  0.826329        youth\n",
       "3541      3  0.772818          zac\n",
       "\n",
       "[330 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 2, 1, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a Dictionary object from the corpus\n",
    "id2wordna = Dictionary.from_corpus(corpusna, id2word=id2wordna)\n",
    "\n",
    "# Enable the notebook mode for displaying the visualization\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Prepare the visualization\n",
    "vis = pyLDAvis.gensim.prepare(ldana, corpusna, id2wordna, mds=\"mmds\", R=30)\n",
    "\n",
    "# Display the visualization\n",
    "vis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: profanity\n",
    "* Topic 1: guns\n",
    "* Topic 2: dogs,tweet\n",
    "* Topic 3: mom, parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Death and other'),\n",
       " (3, 'Jacqueline'),\n",
       " (1, 'Kevin'),\n",
       " (3, 'Taylor'),\n",
       " (0, 'The iron claw')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
    "* Topic 0: profanity[Bill,Kevin,Mike]\n",
    "* Topic 1: guns[Anthony,Jim,Joe,John]\n",
    "* Topic 2: dogs,tweet[Gabriel,Dave,Ricky]\n",
    "* Topic 3: mom, parents[Ali,Louis,Hasan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment:\n",
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Solutions\n",
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.LdaModel(corpus=corpusna, id2word=id2wordna, num_topics=5, passes=100, alpha=0.5, eta=0.01)\n",
    "#alpha refers to prior belief of the topic distribution\n",
    "#eta   refers to prior belief of word distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.029*\"kevin\" + 0.018*\"crowd\" + 0.017*\"kerry\" + 0.016*\"hey\" + 0.015*\"von\" + 0.013*\"champion\" + 0.012*\"david\" + 0.011*\"erich\" + 0.010*\"pam\" + 0.010*\"fritz\"'),\n",
       " (1,\n",
       "  '0.016*\"friends\" + 0.015*\"okay\" + 0.012*\"theyre\" + 0.010*\"single\" + 0.008*\"parents\" + 0.008*\"lot\" + 0.007*\"talk\" + 0.007*\"married\" + 0.006*\"date\" + 0.006*\"shes\"'),\n",
       " (2,\n",
       "  '0.013*\"bit\" + 0.011*\"wee\" + 0.010*\"kev\" + 0.010*\"wife\" + 0.009*\"door\" + 0.008*\"cos\" + 0.008*\"guy\" + 0.007*\"old\" + 0.006*\"son\" + 0.006*\"mate\"'),\n",
       " (3,\n",
       "  '0.022*\"viktor\" + 0.020*\"sams\" + 0.009*\"collier\" + 0.008*\"detective\" + 0.007*\"worlds\" + 0.007*\"factory\" + 0.007*\"hm\" + 0.007*\"imogene\" + 0.007*\"sail\" + 0.006*\"rufus\"'),\n",
       " (4,\n",
       "  '0.023*\"blow\" + 0.022*\"penis\" + 0.021*\"job\" + 0.012*\"cause\" + 0.010*\"sort\" + 0.009*\"teeth\" + 0.007*\"balls\" + 0.007*\"dick\" + 0.005*\"cock\" + 0.005*\"problem\"')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Overall, these parameter adjustments may lead to more diverse and nuanced topics, \n",
    "#with documents and topics being influenced by a wider range of topics and words\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: wife,mom\n",
    "* Topic 1: profanity\n",
    "* Topic 2: gun,kid,religion\n",
    "* Topic 3: dogs,mexican\n",
    "* Topic 4: parents,brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1], 'Death and other'),\n",
       " ([3], 'Jacqueline'),\n",
       " ([1], 'Kevin'),\n",
       " ([3], 'Taylor'),\n",
       " ([0], 'The iron claw')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the topics associated with each document in the corpus\n",
    "topic_assignments = []\n",
    "for doc_topics in corpus_transformed:\n",
    "    topic_assignments.append([topic for topic, _ in doc_topics])\n",
    "\n",
    "# Pairing the topic assignments with document indices\n",
    "list(zip(topic_assignments, data_dtmna.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Topic 0: wife,mom [Anthony,Dave,Joe,John,Kevin]\n",
    "* Topic 1: profanity [Jim,Ricky]\n",
    "* Topic 2: gun,kid,religion [Bill]\n",
    "* Topic 3: dogs,mexican [Gabriel,Mike]\n",
    "* Topic 4: parents,brown [Ali,Hasan,Louis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * ##### Topic Modeling - Attempt #4 (Nouns,Adjectives,AdVerbs and no proper nouns) \n",
    "* ##### [reference](https://sites.google.com/site/partofspeechhelp/home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj_verb(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj_verb = lambda pos: (pos[:2] == 'NN' or pos[:2] == 'JJ' or pos[:2] == 'RB') and pos[:2] != 'NNP'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj_verb = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj_verb(pos)] \n",
    "    return ' '.join(nouns_adj_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Death and other</th>\n",
       "      <td>death other detailsseason episode episode titl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacqueline</th>\n",
       "      <td>get knees novak typical standup comedy show un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kevin</th>\n",
       "      <td>kevin overdue catchup comedy special kevin bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taylor</th>\n",
       "      <td>netflix standup comedy special taylor tomlinso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The iron claw</th>\n",
       "      <td>iron claw sean durkinwritten sean zac efron je...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        transcript\n",
       "Death and other  death other detailsseason episode episode titl...\n",
       "Jacqueline       get knees novak typical standup comedy show un...\n",
       "Kevin            kevin overdue catchup comedy special kevin bri...\n",
       "Taylor           netflix standup comedy special taylor tomlinso...\n",
       "The iron claw    iron claw sean durkinwritten sean zac efron je..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns_adj_verb = pd.DataFrame(data_clean.transcript.apply(nouns_adj_verb))\n",
    "data_nouns_adj_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detailsseason</th>\n",
       "      <th>episode</th>\n",
       "      <th>title</th>\n",
       "      <th>tragicoriginal</th>\n",
       "      <th>release</th>\n",
       "      <th>date</th>\n",
       "      <th>february</th>\n",
       "      <th>hulu</th>\n",
       "      <th>sixth</th>\n",
       "      <th>details</th>\n",
       "      <th>...</th>\n",
       "      <th>teenager</th>\n",
       "      <th>unlikely</th>\n",
       "      <th>activist</th>\n",
       "      <th>landmark</th>\n",
       "      <th>cases</th>\n",
       "      <th>destination</th>\n",
       "      <th>sparks</th>\n",
       "      <th>jaded</th>\n",
       "      <th>playboy</th>\n",
       "      <th>exes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Death and other</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacqueline</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kevin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taylor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The iron claw</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 detailsseason  episode  title  tragicoriginal  release  date  \\\n",
       "Death and other              0        0      0               1        1     0   \n",
       "Jacqueline                   1        3      1               0        1     0   \n",
       "Kevin                        0        0      0               0        1     1   \n",
       "Taylor                       0        0      0               0        0     0   \n",
       "The iron claw                0        0      0               0        1     0   \n",
       "\n",
       "                 february  hulu  sixth  details  ...  teenager  unlikely  \\\n",
       "Death and other         0     0      0        0  ...         0         0   \n",
       "Jacqueline              0     1      1        1  ...         0         1   \n",
       "Kevin                   0     1      1        1  ...         2        11   \n",
       "Taylor                  0     1      2        0  ...         0         1   \n",
       "The iron claw           3     4      0        0  ...         0         0   \n",
       "\n",
       "                 activist  landmark  cases  destination  sparks  jaded  \\\n",
       "Death and other         0         0      0            0       0      0   \n",
       "Jacqueline              0         0      0            1       1      0   \n",
       "Kevin                   1         0      0            0       0      2   \n",
       "Taylor                  0         0      0            0       0      0   \n",
       "The iron claw           0         1      1            0       0      0   \n",
       "\n",
       "                 playboy  exes  \n",
       "Death and other        1     0  \n",
       "Jacqueline             0     2  \n",
       "Kevin                  0     0  \n",
       "Taylor                 0     0  \n",
       "The iron claw          0     0  \n",
       "\n",
       "[5 rows x 4164 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvnav = CountVectorizer(stop_words=a_stop_words, max_df=.8)\n",
    "data_cvnav = cvnav.fit_transform(data_nouns_adj_verb.transcript)\n",
    "data_dtmnav = pd.DataFrame(data_cvnav.toarray(), columns=cvnav.vocabulary_)\n",
    "data_dtmnav.index = data_nouns_adj_verb.index\n",
    "data_dtmnav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusnav = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmnav.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordnav = dict((v, k) for k, v in cvnav.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_nav = models.LdaModel(corpus=corpusnav, id2word=id2wordnav, num_topics=5, passes=100, alpha=0.5, eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*\"blow\" + 0.000*\"kevin\" + 0.000*\"job\" + 0.000*\"bit\" + 0.000*\"penis\" + 0.000*\"guy\" + 0.000*\"crowd\" + 0.000*\"door\" + 0.000*\"cause\" + 0.000*\"okay\"'),\n",
       " (1,\n",
       "  '0.022*\"haha\" + 0.022*\"crisis\" + 0.022*\"wok\" + 0.000*\"bit\" + 0.000*\"door\" + 0.000*\"wee\" + 0.000*\"kevin\" + 0.000*\"wife\" + 0.000*\"cos\" + 0.000*\"guy\"'),\n",
       " (2,\n",
       "  '0.022*\"blow\" + 0.020*\"penis\" + 0.020*\"job\" + 0.011*\"cause\" + 0.009*\"sort\" + 0.008*\"teeth\" + 0.006*\"balls\" + 0.006*\"dick\" + 0.005*\"cock\" + 0.005*\"problem\"'),\n",
       " (3,\n",
       "  '0.013*\"kevin\" + 0.011*\"okay\" + 0.009*\"hey\" + 0.008*\"crowd\" + 0.008*\"kerry\" + 0.007*\"von\" + 0.006*\"friends\" + 0.006*\"forever\" + 0.006*\"champion\" + 0.005*\"david\"'),\n",
       " (4,\n",
       "  '0.012*\"bit\" + 0.011*\"wee\" + 0.010*\"kev\" + 0.009*\"wife\" + 0.009*\"door\" + 0.008*\"cos\" + 0.007*\"guy\" + 0.007*\"old\" + 0.006*\"quite\" + 0.006*\"telly\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_nav.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'Death and other'),\n",
       " (2, 'Jacqueline'),\n",
       " (4, 'Kevin'),\n",
       " (3, 'Taylor'),\n",
       " (3, 'The iron claw')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_transformed = lda_nav[corpusnav]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmnav.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Topic 0: dogs,friend,mexican [Gabriel,Mike]\n",
    "* Topic 1: profanity [Louis]\n",
    "* Topic 2: gun,kid [Ali,Anthony,Bill,Jim,Ricky]\n",
    "* Topic 3: mom,parents,gay [Dave,Hasan]\n",
    "* Topic 4: wife,kid,mom [Joe,John,Kevin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * [reference](https://towardsdatascience.com/latent-dirichlet-allocation-lda-a-guide-to-probabilistic-modeling-approach-for-topic-discovery-8cb97c08da3c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7758e92e9a61d7a3490898707f7eeb937c85e9d1e8d4e877cc6c187218f226d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
